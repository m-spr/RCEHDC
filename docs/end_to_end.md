END_to_END Flow
======

A summary of the proposed FPGA-based platform is presented in \autoref{platOver} I.
The focus of this work is to develop a lightweight FPGA-based accelerator for \ac{HDC} inference in IoT and low-power devices.
Therefore, the training of the network is performed offline using the open-source Python library TorchHD~\cite{JMLR:v24:23-0300}. 
Instead of re-implementing the entire $E^3HDC$ approach within TorchHD, we reuse the existing base-level implementation, filling the BHV and ID-level matrices with the HVs, generated by our hardware-friendly structures.
To adopt TorchHD into our work, two classes are defined to generate the BHV matrix and the ID-level matrix based on our system and are overridden instead of the actual functions in TorchHD for training within TorchHD. 
For the BHV matrix, a recursive Python code is used to optimize the random initial seed and configuration for high orthogonality, as explained in~\autoref{EncodingTechnique}.
This function provides the entire BHV matrix for the learning phase of \ac{HDC}, as well as the configuration and initialization seed for the BHV-Gen module in hardware.
After training, the generated \ac{CHV} are extracted from the trained TorchHD model, and based on segment size \textit{s} initialized in different ROM files in our $E^3HDC$-Python-Framework.
Then, the configuration of the hardware design top module is generated based on the feature size (\textit{f}), dimension size (\textit{d}), and \textit{s}, and loaded into the architecture directory.
Lastly, the design can be synthesized using the vendor-specific FPGA compiler.
As all of the modules are designed to be configurable with the support of VHDL's \textit{recursive} and \textit{for generate} style, there is no need for further modification to hardware codes and they produce correct hardware for any number of features and dimension sizes automatically. 