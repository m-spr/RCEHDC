.. _encoding:
****
E3HDC encoding
****

Most \ac{HDC} encoding approaches rely on one of the following principles: random projection, base level encoding, or permutation.
Here, we explain their basic ideas and their advantages and disadvantages, mainly in terms of memory usage and computational complexity.
The common operations for all approaches are \textit{binding}, where each of the input's $f$ features is transformed into an HV, and \textit{bundling}, where the $f$ generated HVs are summed up and binarized.

SO far E3HDC supports the base level encoding due to its hardware effiecient operations
\textbf{base level} encoding, begins with the use of an ID-level matrix, which maps all possible scalar values of the input feature vector into hyperdimensional space. The ID-level matrix is a static set of binary/bipolar HVs, which are generated to directly translate differences in feature values into similarity in the hyperspace:
* The difference between scalar values must correspond to the dissimilarity of the generated HVs, i.e. two values with a larger difference must be mapped to HVs with greater dissimilarity.
* This ID-level matrix can be generated by choosing the first HV randomly, and 
inferring the remaining HVs by flipping various random partitions of it. 

In this technique,Then the translated values in the hyperdimensional space are multiplied with a static \ac{BHV} matrix to represent the feature positions in the computed HV as: $QHV=binarized(\sum_{k=1}^f ID\_level_{k}\otimes BHV_{k})$ 
In contrast to NNs, where the baseline contains weights that must be learned throughout training, BHV is a one-time pass, i.e., it is formed randomly with binary or bipolar values at the beginning of the training phase and remains constant throughout training and inference. The \acp{BHV} are mostly chosen randomly, but are ensured to be orthogonal, in order to represent each feature independently in the hyperdimensional space.

In this encoding, only element-wise XNOR and summation are needed. However, the method requires memory to store both the ID-level as well as the BHV matrix. Therefore, E3HDC takes advantage of the lightweight computation of this encoding technique and replace the storage with hardware logic in our proposed encoding.

on the fly paramiter generationg
=========
Our lightweight HDC encoding technique, On-the-Fly Generation Encoding (OTFGEncoder), generates encoding parameters on-the-fly at runtime by replacing them with generator circuits. 

--only if we move this to some other pages-- This method is based on the well-known base-level encoding approach (if we are moving this to some other paages refrense-----link--), which requires two large memories for storing the \ac{BHV} matrix and the ID-level matrix. For each input feature, the value is used as the index for the ID-level matrix, and the position of the feature is used as the index for the BHV matrix. The output of these two memories is then XORed. All input features must go through this step and be added together to create the \ac{QHV}. -----


ID-level
------------

In the base-level encoding method, every possible value of the input features must be transferred to the hypervector space before further computations can be performed. We introduce an adaptive system called \textit{ID-Gen} that generates the level hypervectors on-the-fly instead of retrieving them from memory. To form ID-level hypervectors and maintain high classification accuracy, values close to each other are translated to HVs with a higher similarity and vice versa.

Our proposed ID-Gen module, shown in~\autoref{Fig:OTFencoder}-a, is parameterized based on the dimension size \textit{d} of the hypervectors and the range \textit{n} of the quantized features, where features $x$ can take values $x\in [0,n)$: We first choose a repeat value as $AD$ such that $AD = \frac{d}{n}$, and reminder value as $R = {d}\%{n}$.

Then a hypervector is generated for a given value $x$ as foloows:
* first a sequence of $n-(x)$ zeros followed by a sequence of $xAD$ ones is called $S$.
* then the generated $S$ is concatinates with itself  $AD\times$
* then based on the reminder value $R$ the rest of the HV is formed by concatinating $S[R:0] with rest of the HV$ 

In this scenario, a succession of zeros is considered for the lowest potential value of the input features, and the number of ones in the HV increases for higher feature values.
In this approach, the dissimilarity of the HVs correlates directly with the difference between the values of the input vector without the need for memory.


Generating BHVs
---------------

To generate a suitable \ac{BHV} matrix for \ac{HDC}, the orthogonality of the rows in the matrix and the repeatability of the entire matrix for each input must be taken into account. The proposed alternative hardware to replace the BHV matrix memory takes advantage of cyclic architecture circuits that can generate repetitive data sequences with low hardware overhead and is called BHV-Gen.
BHV-Gen combines two flexible cyclic architectures: \ac{LFSR} and \ac{MISR}.
The \ac{LFSR} is a chain of registers that are XORed with the output of the last register and can structure any arbitrary polynomial, e.g., $x^d + x^{d-3} +... + x^0$. The LFSR can generate several sequences with different seed values and different polynomials.
However, the LFSR generates a one-bit output in each clock cycle. The MISR concept, on the other hand, supports parallel output vectors depending on a series of parallel inputs.
Our proposed cyclic BHV-Gen structure combines these concepts as shown in~\autoref{Fig:OTFencoder}-b, where \textit{d} number of registers is connected in a chain and the output of all registers constructs a single row of the BHV matrix. The intuition is that the combination of shifting and feedback loops inherently generates pseudorandom vectors that are also orthogonal. We also confirm this intuition empirically by evaluating more than $10\,000$ different LFSR configurations and seed values with respect to the orthogonality of the generated BHVs.
Considering orthogonality as the dot product of bipolar vectors ($0\rightarrow -1$, $1\rightarrow 1$), the ideal value would be $0$ between all BHVs. In our evaluation, the average orthogonality would be about $0.04$ for simple cyclic shift registers as used in \cite{khaleghi2021tiny}, but about $0.02$ when using our LFSR-like structure.

The BHV-Gen structure generates the BHVs row by row in each clock cycle. We note that this does not limit the throughput, as the whole BHV matrix cannot be loaded from memory within a single clock cycle either. However, multiple rows could be generated simultaneously if multiple LFSRs with the same configuration and different initial seeds are used.


